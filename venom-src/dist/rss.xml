<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"><channel><title>Semiautonomous Systems</title><description>Analysis and research on data poisoning, AI crawler enforcement, and digital accountability infrastructure.</description><link>https://semiautonomous.systems/</link><item><title>Data Poisoning FAQ: Technical, Legal, and Policy Answers</title><link>https://semiautonomous.systems/blog/data-poisoning-faq/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/data-poisoning-faq/</guid><description>Answers to common questions about data poisoning, web crawling, robots.txt, AIPREF, legal status, and enforcement mechanisms for AI training defense.</description><pubDate>Fri, 20 Feb 2026 00:00:00 GMT</pubDate></item><item><title>Publisher Defenses Against AI Scraping: Cost Imposition vs Poisoning</title><link>https://semiautonomous.systems/blog/cost-imposition-vs-value-degradation/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/cost-imposition-vs-value-degradation/</guid><description>Comparing defense strategies against AI scraping: proof-of-work systems impose costs, data poisoning degrades value. Who pays and what works for publishers.</description><pubDate>Tue, 17 Feb 2026 00:00:00 GMT</pubDate></item><item><title>AI Poisoning Threat Models: Backdoors, RAG, and Supply Chain</title><link>https://semiautonomous.systems/blog/threat-models-data-poisoning/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/threat-models-data-poisoning/</guid><description>Backdoor attacks, model degradation, and RAG poisoning explained. Technical analysis of who can attack, defense costs, and power dynamics in AI training data.</description><pubDate>Fri, 13 Feb 2026 00:00:00 GMT</pubDate></item><item><title>Defensive Data Poisoning: Ethics, Risks, and Alternatives</title><link>https://semiautonomous.systems/blog/defensive-poisoning-ethics/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/defensive-poisoning-ethics/</guid><description>Analyzing ethical tradeoffs of defensive data poisoning: proportionality, collateral damage, and safer alternatives like proof-of-work and AIPREF standards.</description><pubDate>Tue, 10 Feb 2026 00:00:00 GMT</pubDate></item><item><title>What Is Data Poisoning in Machine Learning?</title><link>https://semiautonomous.systems/blog/what-is-data-poisoning/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/what-is-data-poisoning/</guid><description>Data poisoning manipulates AI training data to alter model behavior. Learn how defensive tools like Nightshade protect content from unauthorized AI training.</description><pubDate>Sat, 07 Feb 2026 00:00:00 GMT</pubDate></item><item><title>Why VENOM Exists: From robots.txt to AI Data Enforcement</title><link>https://semiautonomous.systems/blog/why-venom-exists/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/why-venom-exists/</guid><description>When robots.txt fails, enforcement mechanisms emerge. VENOM analyzes data poisoning, proof-of-work, and technical countermeasures for AI training governance.</description><pubDate>Thu, 05 Feb 2026 00:00:00 GMT</pubDate></item><item><title>The State of Defensive Data Poisoning in 2026: A Report</title><link>https://semiautonomous.systems/blog/state-of-data-poisoning-2026/</link><guid isPermaLink="true">https://semiautonomous.systems/blog/state-of-data-poisoning-2026/</guid><description>Comprehensive analysis of AI training data enforcement: robots.txt bypass data, tool effectiveness, legal developments, and the shift from signaling to enforcement.</description><pubDate>Tue, 03 Feb 2026 00:00:00 GMT</pubDate></item></channel></rss>